{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TASK:\n",
        "1. Take any csv/text file of 2+ GB of your choice. --- (You can do this assignment on Google colab)\n",
        "\n",
        "2. Read the file ( Present approach of reading the file )\n",
        "\n",
        "3. Try different methods of file reading eg: Dask, Modin, Ray, pandas and present your findings in term of computational efficiency\n",
        "\n",
        "4. Perform basic validation on data columns : eg: remove special character , white spaces from the col name\n",
        "\n",
        "5. As you already know the schema hence create a YAML file and write the column name in YAML file. --define separator of read and write file, column name in YAML\n",
        "\n",
        "6. Validate number of columns and column name of ingested file with YAML.\n",
        "\n",
        "7. Write the file in pipe separated text file (|) in gz format.\n",
        "\n",
        "8. Create a summary of the file:\n",
        "\n",
        "- Total number of rows,\n",
        "\n",
        "- total number of columns\n",
        "\n",
        "- file size"
      ],
      "metadata": {
        "id": "8J3yjncet5L9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###########################\n",
        "# GENERATING 2GB CSV FILE #\n",
        "###########################\n",
        "\n",
        "import csv\n",
        "import os\n",
        "import random\n",
        "import string\n",
        "\n",
        "SIZE_LIMIT = 2 * 1024 * 1024 * 1024\n",
        "output_file = \"/content/test_2GB.csv\"\n",
        "\n",
        "def random_string(length=10):\n",
        "    \"\"\"Generate a random string of fixed length.\"\"\"\n",
        "    return ''.join(random.choices(string.ascii_letters, k=length))\n",
        "\n",
        "def random_float(min_val=0, max_val=1000):\n",
        "    \"\"\"Generate a random float rounded to 2 decimal places.\"\"\"\n",
        "    return round(random.uniform(min_val, max_val), 2)\n",
        "\n",
        "# Generate CSV file\n",
        "with open(output_file, 'w', newline='', buffering=1024*1024) as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"ID\", \"Name\", \"Value\"])\n",
        "\n",
        "    rows_written = 0\n",
        "    while True:\n",
        "        rid = rows_written + 1\n",
        "        name = random_string(12)\n",
        "        val = random_float(10, 1000)\n",
        "\n",
        "        writer.writerow([rid, name, val])\n",
        "        rows_written += 1\n",
        "\n",
        "        if rows_written % 100000 == 0:\n",
        "            f.flush()\n",
        "            current_size = os.stat(output_file).st_size\n",
        "            print(f\"Rows Written: {rows_written:,}, File Size: {current_size / (1024 * 1024):.2f} MB\", flush=True)\n",
        "            if current_size >= SIZE_LIMIT:\n",
        "                print(\"Reached target file size limit.\")\n",
        "                break\n",
        "\n",
        "print(\"File generation complete:\", output_file)\n"
      ],
      "metadata": {
        "id": "JamGxYaauGCo",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Get the size of the file\n",
        "file_path = \"test_2GB.csv\"\n",
        "\n",
        "file_size = os.path.getsize(file_path)\n",
        "\n",
        "file_size_mb = file_size / (1024 * 1024)\n",
        "file_size_gb = file_size / (1024 * 1024 * 1024)\n",
        "\n",
        "print(f\"File Size: {file_size:.2f} bytes\")\n",
        "print(f\"File Size: {file_size_mb:.2f} MB\")\n",
        "print(f\"File Size: {file_size_gb:.2f} GB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykKJSRue5BWO",
        "outputId": "210819c6-943b-42a7-99ff-69fec310af17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File Size: 2150048739.00 bytes\n",
            "File Size: 2050.45 MB\n",
            "File Size: 2.00 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile testutility.py\n",
        "import logging\n",
        "import os\n",
        "import subprocess\n",
        "import yaml\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import gc\n",
        "import re\n",
        "\n",
        "\n",
        "################\n",
        "# File Reading #\n",
        "################\n",
        "\n",
        "def read_config_file(filepath):\n",
        "    with open(filepath, 'r') as stream:\n",
        "        try:\n",
        "            return yaml.safe_load(stream)\n",
        "        except yaml.YAMLError as exc:\n",
        "            logging.error(exc)\n",
        "\n",
        "\n",
        "def replacer(string, char):\n",
        "    pattern = char + '{2,}'\n",
        "    string = re.sub(pattern, char, string)\n",
        "    return string\n",
        "\n",
        "def col_header_val(df,table_config):\n",
        "    '''\n",
        "    replace whitespaces in the column\n",
        "    and standardized column names\n",
        "    '''\n",
        "    df.columns = df.columns.str.lower()\n",
        "    df.columns = df.columns.str.replace('[^\\w]','_',regex=True)\n",
        "    df.columns = list(map(lambda x: x.strip('_'), list(df.columns)))\n",
        "    df.columns = list(map(lambda x: replacer(x,'_'), list(df.columns)))\n",
        "    expected_col = list(map(lambda x: x.lower(),  table_config['columns']))\n",
        "    expected_col.sort()\n",
        "    df.columns =list(map(lambda x: x.lower(), list(df.columns)))\n",
        "    df = df.reindex(sorted(df.columns), axis=1)\n",
        "    if len(df.columns) == len(expected_col) and list(expected_col)  == list(df.columns):\n",
        "        print(\"column name and column length validation passed\")\n",
        "        return 1\n",
        "    else:\n",
        "        print(\"column name and column length validation failed\")\n",
        "        mismatched_columns_file = list(set(df.columns).difference(expected_col))\n",
        "        print(\"Following File columns are not in the YAML file\",mismatched_columns_file)\n",
        "        missing_YAML_file = list(set(expected_col).difference(df.columns))\n",
        "        print(\"Following YAML columns are not in the file uploaded\",missing_YAML_file)\n",
        "        logging.info(f'df columns: {df.columns}')\n",
        "        logging.info(f'expected columns: {expected_col}')\n",
        "        return 0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daZd2yOHICfz",
        "outputId": "e8dd9105-70b3-4115-d601-d41685c59223"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing testutility.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write the YAML File"
      ],
      "metadata": {
        "id": "nOG8q9EkIKxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile config.yaml\n",
        "file_type: csv\n",
        "dataset_name: testfile\n",
        "file_name: test_2GB\n",
        "table_name: test_table\n",
        "inbound_delimiter: \",\"\n",
        "outbound_delimiter: \"|\"\n",
        "skip_leading_rows: 1\n",
        "columns:\n",
        "    - id\n",
        "    - name\n",
        "    - value"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfaFXpGVIMsH",
        "outputId": "c09c8488-0473-49a5-ebd0-8c6af19058bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing config.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read config file\n",
        "import testutility as util\n",
        "config_data = util.read_config_file(\"config.yaml\")\n",
        "print(config_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWBEjE9fJMH0",
        "outputId": "d3ee9902-6541-400b-c6e8-1cc656bda52c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'file_type': 'csv', 'dataset_name': 'testfile', 'file_name': 'test_2GB', 'table_name': 'test_table', 'inbound_delimiter': ',', 'outbound_delimiter': '|', 'skip_leading_rows': 1, 'columns': ['id', 'name', 'value']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config_data['inbound_delimiter']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "uf7N339eJaVN",
        "outputId": "80b7b078-84df-4496-e8ca-d5d59c88ebb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "','"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxjOW-_cJbw0",
        "outputId": "87b187c5-d532-4a6d-871d-ba25658b16ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'file_type': 'csv',\n",
              " 'dataset_name': 'testfile',\n",
              " 'file_name': 'test_2GB',\n",
              " 'table_name': 'test_table',\n",
              " 'inbound_delimiter': ',',\n",
              " 'outbound_delimiter': '|',\n",
              " 'skip_leading_rows': 1,\n",
              " 'columns': ['id', 'name', 'value']}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "import subprocess\n",
        "import yaml\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import gc\n",
        "import re\n",
        "import dask.dataframe as dd\n",
        "import time"
      ],
      "metadata": {
        "id": "jB3jyf5tMSvh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dask[complete] modin[ray] pyyaml"
      ],
      "metadata": {
        "id": "DvuHQgN2JjC4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "018d5a41-ab4c-4645-a654-fe659abd0355"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (6.0.2)\n",
            "Requirement already satisfied: dask[complete] in /usr/local/lib/python3.10/dist-packages (2024.10.0)\n",
            "Requirement already satisfied: modin[ray] in /usr/local/lib/python3.10/dist-packages (0.32.0)\n",
            "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.10/dist-packages (from dask[complete]) (8.1.7)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]) (3.1.0)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]) (24.2)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]) (1.4.2)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]) (8.5.0)\n",
            "Requirement already satisfied: pyarrow>=14.0.1 in /usr/local/lib/python3.10/dist-packages (from dask[complete]) (17.0.0)\n",
            "Collecting lz4>=4.3.2 (from dask[complete])\n",
            "  Downloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: pandas<2.3,>=2.2 in /usr/local/lib/python3.10/dist-packages (from modin[ray]) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from modin[ray]) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from modin[ray]) (5.9.5)\n",
            "Requirement already satisfied: ray!=2.5.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from modin[ray]) (2.40.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask[complete]) (3.21.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3,>=2.2->modin[ray]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3,>=2.2->modin[ray]) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3,>=2.2->modin[ray]) (2024.2)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.10/dist-packages (from partd>=1.4.0->dask[complete]) (1.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray!=2.5.0,>=2.1.0->modin[ray]) (3.16.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray!=2.5.0,>=2.1.0->modin[ray]) (4.23.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray!=2.5.0,>=2.1.0->modin[ray]) (1.1.0)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray!=2.5.0,>=2.1.0->modin[ray]) (4.25.5)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray!=2.5.0,>=2.1.0->modin[ray]) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray!=2.5.0,>=2.1.0->modin[ray]) (1.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray!=2.5.0,>=2.1.0->modin[ray]) (2.32.3)\n",
            "Requirement already satisfied: bokeh>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from dask[complete]) (3.6.2)\n",
            "Requirement already satisfied: jinja2>=2.10.3 in /usr/local/lib/python3.10/dist-packages (from dask[complete]) (3.1.4)\n",
            "Collecting dask-expr<1.2,>=1.1 (from dask[complete])\n",
            "  Downloading dask_expr-1.1.20-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting distributed==2024.10.0 (from dask[complete])\n",
            "  Downloading distributed-2024.10.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting sortedcontainers>=2.0.5 (from distributed==2024.10.0->dask[complete])\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting tblib>=1.6.0 (from distributed==2024.10.0->dask[complete])\n",
            "  Downloading tblib-3.0.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: tornado>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.10.0->dask[complete]) (6.3.3)\n",
            "Requirement already satisfied: urllib3>=1.26.5 in /usr/local/lib/python3.10/dist-packages (from distributed==2024.10.0->dask[complete]) (2.2.3)\n",
            "Collecting zict>=3.0.0 (from distributed==2024.10.0->dask[complete])\n",
            "  Downloading zict-3.0.0-py2.py3-none-any.whl.metadata (899 bytes)\n",
            "Requirement already satisfied: contourpy>=1.2 in /usr/local/lib/python3.10/dist-packages (from bokeh>=3.1.0->dask[complete]) (1.3.1)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from bokeh>=3.1.0->dask[complete]) (11.0.0)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.10/dist-packages (from bokeh>=3.1.0->dask[complete]) (2024.9.0)\n",
            "INFO: pip is looking at multiple versions of dask-expr to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting dask-expr<1.2,>=1.1 (from dask[complete])\n",
            "  Downloading dask_expr-1.1.19-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading dask_expr-1.1.18-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading dask_expr-1.1.16-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.10.3->dask[complete]) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<2.3,>=2.2->modin[ray]) (1.16.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray!=2.5.0,>=2.1.0->modin[ray]) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray!=2.5.0,>=2.1.0->modin[ray]) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray!=2.5.0,>=2.1.0->modin[ray]) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray!=2.5.0,>=2.1.0->modin[ray]) (0.22.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ray!=2.5.0,>=2.1.0->modin[ray]) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray!=2.5.0,>=2.1.0->modin[ray]) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray!=2.5.0,>=2.1.0->modin[ray]) (2024.8.30)\n",
            "Downloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading distributed-2024.10.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dask_expr-1.1.16-py3-none-any.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.2/243.2 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Downloading tblib-3.0.0-py3-none-any.whl (12 kB)\n",
            "Downloading zict-3.0.0-py2.py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/43.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sortedcontainers, zict, tblib, lz4, distributed, dask-expr\n",
            "Successfully installed dask-expr-1.1.16 distributed-2024.10.0 lz4-4.3.3 sortedcontainers-2.4.0 tblib-3.0.0 zict-3.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reading with Pandas"
      ],
      "metadata": {
        "id": "-hPH3vZFLx-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "pandas_df = pd.read_csv(\"/content/test_2GB.csv\", delimiter=config_data['inbound_delimiter'], skiprows=config_data['skip_leading_rows'], header=None, names=config_data['columns'])\n",
        "pandas_read_time = time.time() - start_time\n",
        "print(\"Pandas read time:\", pandas_read_time, \"seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTQvI_rGLz5-",
        "outputId": "9a0a8d0a-930c-4a0e-b2ad-e73084f3233d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pandas read time: 217.16818022727966 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reading with Dask"
      ],
      "metadata": {
        "id": "8ZhEteOTOXiu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dask.dataframe as dd\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "dask_df = dd.read_csv(\n",
        "    \"/content/test_2GB.csv\",\n",
        "    delimiter=config_data['inbound_delimiter'],\n",
        "    header=0,\n",
        "    assume_missing=True\n",
        ")\n",
        "\n",
        "_ = dask_df.head()\n",
        "\n",
        "# Step 4: Measure elapsed time\n",
        "elapsed_time = time.time() - start_time\n",
        "print(f\"Dask read time (including head computation): {elapsed_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpG3NyJxOfJH",
        "outputId": "25d0174e-6f4d-401e-e6dc-eb6c6f0c0b1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dask read time (including head computation): 10.50 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Validation"
      ],
      "metadata": {
        "id": "X3vNxFPZPYZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate columns with pandas\n",
        "col_status = util.col_header_val(pandas_df, config_data)\n",
        "if col_status:\n",
        "    print(\"Validation successful with pandas dataframe.\")\n",
        "else:\n",
        "    print(\"Validation failed with pandas dataframe.\")\n"
      ],
      "metadata": {
        "id": "nmOyW1-LMhW2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48738eac-4633-45cf-a60c-2269997dd22b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "column name and column length validation passed\n",
            "Validation successful with pandas dataframe.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "\n",
        "input_path = \"/content/test_2GB.csv\"\n",
        "output_path = \"/content/test_2GB_out.txt.gz\"\n",
        "\n",
        "config_data = util.read_config_file(\"config.yaml\")\n",
        "expected_col = [c.lower() for c in config_data['columns']]\n",
        "expected_col.sort()\n",
        "\n",
        "chunk_size = 10000000\n",
        "\n",
        "print(\"Processing and writing in chunks...\")\n",
        "with gzip.open(output_path, 'wt', newline='') as gz_file:\n",
        "    header_written = False\n",
        "\n",
        "    for chunk in pd.read_csv(\n",
        "        input_path,\n",
        "        delimiter=config_data['inbound_delimiter'],\n",
        "        header=0,\n",
        "        chunksize=chunk_size\n",
        "    ):\n",
        "        chunk.columns = (\n",
        "            chunk.columns.str.lower()\n",
        "            .str.replace(r'[^\\w]', '_', regex=True)\n",
        "            .str.strip('_')\n",
        "        )\n",
        "\n",
        "        missing_cols = set(expected_col) - set(chunk.columns)\n",
        "        if missing_cols:\n",
        "            raise ValueError(f\"Missing columns in chunk: {missing_cols}\")\n",
        "\n",
        "        chunk = chunk[expected_col]\n",
        "\n",
        "        chunk.to_csv(\n",
        "            gz_file,\n",
        "            sep=config_data['outbound_delimiter'],  # Pipe-separated\n",
        "            index=False,\n",
        "            header=not header_written\n",
        "        )\n",
        "        header_written = True\n",
        "\n",
        "        print(f\"Processed chunk of size {len(chunk)} rows.\")\n",
        "\n",
        "print(f\"File written successfully to {output_path}\")\n",
        "print(f\"Output file size: {os.path.getsize(output_path) / (1024 * 1024):.2f} MB\")\n"
      ],
      "metadata": {
        "id": "xvFgLr2LPX11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02f60ae9-059f-4185-f8f6-5d8eec5fcb09"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing and writing in chunks...\n",
            "Processed chunk of size 10000000 rows.\n",
            "Processed chunk of size 10000000 rows.\n",
            "Processed chunk of size 10000000 rows.\n",
            "Processed chunk of size 10000000 rows.\n",
            "Processed chunk of size 10000000 rows.\n",
            "Processed chunk of size 10000000 rows.\n",
            "Processed chunk of size 10000000 rows.\n",
            "Processed chunk of size 2500000 rows.\n",
            "File written successfully to /content/test_2GB_out.txt.gz\n",
            "Output file size: 1140.53 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary of the file"
      ],
      "metadata": {
        "id": "3zNG7ZU0Y4lN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_path = \"/content/test_2GB.csv\"\n",
        "\n",
        "file_size = os.path.getsize(input_path) / (1024 * 1024 * 1024)\n",
        "\n",
        "print(\"Reading the file with Dask...\")\n",
        "dask_df = dd.read_csv(\n",
        "    input_path,\n",
        "    delimiter=config_data['inbound_delimiter'],\n",
        "    header=0\n",
        ")\n",
        "\n",
        "print(\"Computing total rows and columns...\")\n",
        "total_rows = dask_df.shape[0].compute()\n",
        "total_cols = len(dask_df.columns)\n",
        "\n",
        "# Summary\n",
        "print(\"Summary of the original file:\")\n",
        "print(\"Total number of rows:\", total_rows)\n",
        "print(\"Total number of columns:\", total_cols)\n",
        "print(f\"File size: {file_size:.2f} GB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsPfOEbgY7G9",
        "outputId": "def45b99-0ffc-40f8-bc33-1651d23cda7e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading the file with Dask...\n",
            "Computing total rows and columns...\n",
            "Summary of the original file:\n",
            "Total number of rows: 72500000\n",
            "Total number of columns: 3\n",
            "File size: 2.00 GB\n"
          ]
        }
      ]
    }
  ]
}